{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Thursday Harbor: SciPy Tutorial </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Mandatory package imports\n",
    "import scipy\n",
    "import scipy.signal as signal\n",
    "import scipy.ndimage as ndi\n",
    "import scipy.stats as stats\n",
    "import scipy.optimize as optimize\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# display initialization\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Optional imports\n",
    "import skimage.feature\n",
    "import rpy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Make sure all data files are available\n",
    "import os\n",
    "for f in ['MAX_ZSeries-ch1.tif', 'MAX_ZSeries-ch2.tif', 'noisy_signal.h5', \n",
    "          'power_calibrations.csv', 'sweeps.h5', 'BOb.png']:\n",
    "    if not os.path.isfile(f):\n",
    "        print 'please download %s' %(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Outline </h2>\n",
    "\n",
    "1. Curve fitting and optimization using scipy.optimize\n",
    "2. Signal Processing and 1D Filtering using scipy.signal\n",
    "4. 2D (image) loading and filtering using scipy.ndimage and scipy.signal\n",
    "4. Some statistics using scipy.stats\n",
    "5. Optional material/exercise (time permitting)\n",
    "\n",
    "Note that during all of these modules, there will be examples of various ways to load/save and visualize data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Curve Fitting & Optimization </h2>\n",
    "\n",
    "Many of the functions we are interested in are nonlinear. A nonlinear least squares fit is an iterative process to minimize the error function (residuals) of the fit to the data provided. This is why a nonlinear least squares fit, unlike a regression, needs an initial guess for the parameters to be optimized. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a simple example: In laser microscopy, we control the power of a laser by sending it a command voltage (via a Pockels cell). As we increase the command voltage, the laser outputs more power. However, the voltage/power relationship is nonlinear, so we will attempt to find an analytic equation that describes this relationship by fitting a sine wave to the measured data. \n",
    "\n",
    "The data are saved as columns in a .csv file. We will use the Pandas library to quickly load this data and retain its tabular format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#load up the data file here\n",
    "power_calib_file = 'power_calibrations.csv'\n",
    "power_df = pd.read_csv(power_calib_file)\n",
    "power_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Pandas library also contains some \"smart\" (and quick) plotting functions, where we can tell the plot function which column of the dataframe we want on each axis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "power_df.plot(x='PockelVoltage', y='FidelityPowerPerArea', kind='scatter');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've written two important functions below: \n",
    "\n",
    "* `sine(params, x)` generates a sine wave using 4 parameters that set the scale/offset of the wave, and either a numpy array or scalar of input data. We want to fit this function to our data above by determining the correct set of parameters. The return value(s) are given by the equation:\n",
    "\n",
    "$$\n",
    "f(x) = A  sin(B (x-C)) + D\n",
    "$$\n",
    "\n",
    "\n",
    "* `err_fxn(params, x, y)` defines the error function to be minized by SciPy's `least_squares` functions. It is simply the difference (residuals) of the measured data and sine wave function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def sine(params, x):\n",
    "    \"\"\" Model function determines the shape of the curve to fit.\n",
    "    \"\"\"\n",
    "    A, B, C, D = params\n",
    "    return A * np.sin(B * (x-C)) + D\n",
    "\n",
    "def err_fxn(params, x, y):\n",
    "    \"\"\" Error function that returns the difference between the measured\n",
    "    values and the predicted values given by sine(x)\n",
    "    \"\"\"\n",
    "    return y - sine(params, x)\n",
    "\n",
    "\n",
    "p0 = (30, 0.2, 0.5, 30) # initial guess\n",
    "\n",
    "voltage_input = power_df.PockelVoltage.values # input to the fitting function\n",
    "measured_power = power_df.FidelityPowerPerArea.values # real data used to find fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SciPy's `optimize.least_squares` function takes two required inputs -- the error function and the initial parameters -- as well as any additional arguments that need to be passed into the error function in proper order [in our case, the input data (voltage_input) and the measured data (measured_power)]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fit = optimize.least_squares(err_fxn, p0, args = (voltage_input, measured_power))\n",
    "\n",
    "print fit.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fit parameters tell us our analytic equation should take the following form:\n",
    "\n",
    "$$\n",
    "f(x) = 32.7  sin(1.94 (x-0.79)) + 32.3\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's qualitatively see how good the parameter estimation is by passing the fit parameters into the sigmoid function and plotting it on the same axis as our measured results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sine_fit = sine(fit.x, voltage_input) #pass our optimized fit parameters and input into the sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "########### PLOTTING #############\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "data = ax.plot(voltage_input, measured_power, 'o')\n",
    "fit_data = ax.plot(voltage_input, sine_fit, color='r', linewidth=2)\n",
    "plt.legend(['data', 'fit'])\n",
    "ax.set_xlabel('Voltage')\n",
    "ax.set_ylabel('mW/um2');\n",
    "##################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SOME ADDITIONAL TIPS:**\n",
    "\n",
    "If `optimize.least_squares` is not coverging on a reasonable solution, consider adjusting your initial parameter guess. A good way to do that would be to plot several options for each parameter and qualitatively look for a reaosnable fit to your data. Pass that guess into `least_squares` and see if a minimum can be found. \n",
    "\n",
    "Next, `optimize.least_squares` takes some additional arguments that can give you more control over the fitting. You can set bounds to constrain the minimization region, or the minimzation method. If computation time is an issue, the Jaocbian matrix (a matrix of the partial derivatives of each component of fit) can be defined and passed into `optimize.least_squares`. \n",
    "\n",
    "See more here: https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.least_squares.html\n",
    "\n",
    "Additionally, `optimize.minimize` has similar functionality, with more freedom to set various bounds and methods for optimization. `optimize.curve_fit` contains a built in error function, so it is not necessary to define this, but only the fit function, when using this module.\n",
    "\n",
    "https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html<br>\n",
    "https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** EXERCISE: FIT AN EXPONENTIAL TO A PHOTOBLEACHED CALCIUM TRACE AND REMOVE THIS NONLINEAR BASELINE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "################ EXERCISE ####################\n",
    "# Let's load up a noisy trace using h5py\n",
    "# Note that by using \"with\" instead of file.open(), the file is automatically closed once we are completed\n",
    "\n",
    "with h5py.File('noisy_signal.h5', 'r') as f:\n",
    "    noisy_ca_trace = np.array(f['signal']) #path to trace\n",
    "\n",
    "sample_rate = 30 #Hz\n",
    "step_size = 1.0 / sample_rate\n",
    "time_vec = np.arange(len(noisy_ca_trace)) * step_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An exponential should take the following form:\n",
    "\n",
    "$$\n",
    "Y = A*{e}^{Bt} + D\n",
    "$$\n",
    "\n",
    "Note that your initial guess for B should be negative for a decaying exponential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "############### EXERCISE CONT #################\n",
    "# Suggestions: \n",
    "# 1) write a function that takes a tuple of parameters and an array\n",
    "#    of time values, and returns an exponential \n",
    "# 2) optimize your parameter guess using optimize.least_squares()\n",
    "# 3) subtract the exponential fit from the trace \n",
    "\n",
    "def exponential(params, t):\n",
    "    \"\"\"define exp_fxn here\"\"\"\n",
    "    return exp_fxn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Signal Processing </h2>\n",
    "\n",
    "By digitally sampling a signal, we are turning a continuous signal into a discrete, finite signal, and thus introduce several limitations into how we approach extracting meaning from data. The most obvious is the sampling rate: we cannot expect to detect any periodic signal that occurs faster than half the sampling rate (Nyquist frequency) due to the potential for aliasing errors. The sampling rate thus plays a big role in the detecting frequencies in data and applying any desired filters. \n",
    "\n",
    "Fourier analysis is the cornerstone of DSP. It is predicated upon the theory that any signal can be written as a sum of an infinite number of cosines and sines of different frequency, amplitude, and phase. Applicable to signal analysis, is the ability to bring a signal into the Fourier domain, and see what frequencies are present in the signal (and at what powers). From there, we can try to figure out where to properly apply filters to attenuate any noise. \n",
    "\n",
    "The Fast Fourier Transform (FFT) is the algorithm most commonly used to perform this analysis on a finite, sampled signal. We will start by examining the FFT of the baseline subtracted trace we calculated in the above exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# If you did not manage to subtract the baseline from the calcium\n",
    "# signal above, then you can load ours:\n",
    "with h5py.File('noisy_signal.h5', 'r') as f:\n",
    "    baseline_subtracted_ca = np.array(f['baseline_subtracted_signal'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# First, calculate the discrete fourier transform of the signal.\n",
    "ft = scipy.fftpack.fft(baseline_subtracted_ca)\n",
    "\n",
    "# Notice that the shape of this array is the same, but the dtype\n",
    "# has changed:\n",
    "print \"shape: \", baseline_subtracted_ca.shape, \" => \", ft.shape\n",
    "print \"dtype: \", baseline_subtracted_ca.dtype, \" => \", ft.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# fft() returns an array of complex values; here we use abs(ft) to get  \n",
    "# the _magnitude_, since we are only interested in the power of each \n",
    "# frequency, not the phase (and in any case, we can't plot complex\n",
    "# numbers so we needed to convert them to real values somehow).\n",
    "abs_ft = abs(ft)\n",
    "print abs_ft.dtype\n",
    "\n",
    "# Let's see what this array looks like:\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(abs_ft);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Notice that the right half of the fourier transform is a mirror image\n",
    "# of the left side. Since we started with an input array of real values\n",
    "# (not complex), it is safe to discard this data:\n",
    "pos_ft = abs_ft[:len(abs_ft)//2]\n",
    "\n",
    "# If we imagine our original signal being composed of many different sine waves\n",
    "# added together, then each value in abs_ft tells us the amplitude of a particular\n",
    "# frequency of sine wave present in the original signal. \n",
    "# The fftfreq() function returns an array of the frequencies for each value in abs_ft:\n",
    "freqs = scipy.fftpack.fftfreq(len(baseline_subtracted_ca), step_size)\n",
    "pos_freqs = freqs[:len(abs_ft)//2]  # clip these values as well\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(pos_freqs, pos_ft)\n",
    "plt.xlabel('Frequency (Hz)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it looks like we have some low frequency noise in the 0-1Hz range, and high frequency noise above 4Hz. Our signal is buried in there somewhere (hopefully in the quiet 1-3Hz range).\n",
    "\n",
    "---------------------------\n",
    "\n",
    "Now that we know what frequency components exist in the signal, we can design filters to attenuate any specific frequencies we do not want, or broadband noise in general. There are numerous filter options, and choosing one is usually a trade off between computation time, ripple in the stopband/passband, and the desire for recursive/nonrecursive methods. In this example, we will implement the commonly used Butterworth filter, which has a flat pass band and approaches ideal as the order is increased. \n",
    "\n",
    "Most of SciPy's tools for processing 1D signal data are found in the `signal` subpackage:\n",
    "https://docs.scipy.org/doc/scipy-0.16.1/reference/signal.html\n",
    "\n",
    "\n",
    "If you would like more details on other filter options, the following is a great resource (although for MATLAB, analagous options exist in Python):\n",
    "\n",
    "https://www.mathworks.com/help/signal/examples/practical-introduction-to-digital-filter-design.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Lets design a low pass filter to reject high frequency noise above 5Hz\n",
    "cutoff_freq = 5\n",
    "\n",
    "# The behavior of our filter is determined by two parameters (b, a) which\n",
    "# can be computed using the butter() function:\n",
    "\n",
    "# The cutoff frequency must be mapped [0,1]; the highest cutoff possible (fs/2) is 1\n",
    "low = cutoff_freq / (0.5 * sample_rate) \n",
    "# larger values for order create a sharper cutoff\n",
    "order = 7  \n",
    "b, a = signal.butter(order, [low], btype='low')\n",
    "\n",
    "# With parameters (b, a), we can plot the frequency response of this filter \n",
    "w, h = signal.freqz(b, a)\n",
    "filter_freq_axis  = 1 / (2 * step_size * np.pi) * w\n",
    "filter_response = abs(h)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(pos_freqs, pos_ft / pos_ft.max())\n",
    "plt.plot(filter_freq_axis, filter_response, 'red');\n",
    "plt.xlabel('Frequency (Hz)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Finally, we can filter our signal and see the result:\n",
    "filtered_signal = signal.filtfilt(b, a, baseline_subtracted_ca) # *see note below\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(time_vec, baseline_subtracted_ca)\n",
    "plt.plot(time_vec, filtered_signal);\n",
    "plt.xlabel('s');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Getting close! Let's see what the frequency spectrum looks like now:\n",
    "filtered_ft = abs(scipy.fftpack.fft(filtered_signal))[:len(filtered_signal)//2]\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(pos_freqs, pos_ft)\n",
    "plt.plot(pos_freqs, filtered_ft);\n",
    "plt.xlabel('Frequency (Hz)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** *NOTE: ** There are in fact two commonly used functions to filter you signal: `signal.lfilter` and `signal.filtfilt`. The former applies the filter once, and therefore introduces a phase shift, while the latter does both a forward and backward filter application to eliminate the phase shift (but increases computation time)\n",
    "\n",
    "-------------------\n",
    "\n",
    "We saw in the above example what a filter looks like in the Fourier domain. In fact, the method of applying a filter differs in time and frequency. In the Fourier domain, we could imagine simply multiplying the signal and the filter, attenuating everything outside the passband, and retaining everything within the passband. This multiplication in frequency is actually a different operation -- convolution -- in time.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** EXERCISE: FURTHER FILTER THE ABOVE SIGNAL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "############### EXERCISE ###############\n",
    "# suggestions: \n",
    "# 1) Adjust the lowpass cutoff frequency to reject the remaining high frequency noise\n",
    "# 2) Implement a high pass filter to eliminate some low-frequency oscillations\n",
    "#    (note: the final argument to the butter() function can be either 'low' or 'high')\n",
    "# 3) Maybe there is a more clever way to remove the LF noise?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Image Processing & 2D Filtering</h2>\n",
    "\n",
    "Let's start by loading in the red and green channels of a two photon image, and convincing ourselves that images are really just matrices of pixel values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "im_ch1 = ndi.imread('MAX_ZSeries-ch1.tif')\n",
    "im_ch2 = ndi.imread('MAX_ZSeries-ch2.tif')\n",
    "print im_ch1.shape, type(im_ch1), im_ch1.max(), im_ch1.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a colorful RGB image, we need to merge these two channels. To do that, let's create a numpy array full of zeros that has the same width and height as our channel arrays, but a depth of 3 (R, G, B). We can assign channel 1 (red) to the first stack, and channel 2 (green) to the second. Matplotlib has built in functions for displaying grayscale and rgb images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# How to merge color channels\n",
    "rgb_im = np.zeros((im_ch1.shape[0], im_ch1.shape[1], 3), 'float32')\n",
    "rgb_im[...,0] = im_ch1\n",
    "rgb_im[...,1] = im_ch2\n",
    "\n",
    "#For RGB images of float dtype, matplotlib requires values to be between 0-1; so we normalize to the max\n",
    "rgb_im = rgb_im / rgb_im.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "########## DISPLAY ###########\n",
    "fig_image = plt.figure(figsize=(15, 10))\n",
    "ax1 = fig_image.add_subplot(131)\n",
    "ax2 = fig_image.add_subplot(132)\n",
    "ax3 = fig_image.add_subplot(133)\n",
    "\n",
    "ax1.imshow(im_ch1, cmap=plt.cm.gray)\n",
    "ax2.imshow(im_ch2, cmap=plt.cm.gray)\n",
    "ax3.imshow(rgb_im) ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2D Filtering**\n",
    "\n",
    "Image filtering often begins with the same approach as with 1D signals--we use some kind of low-pass filter to remove noise, and some kind of high-pass filter to remove the baseline. We could, in principle, apply the same tools we used above to filtering 2D image data: we can decompose the image into its frequency components and design filters with specific bandpass characteristics. However, is is often the case that with images, the interesting data shows up in the _phase_ rather than the _frequency_. Thus, we take a somewhat different approach to handling this type of signal.\n",
    "\n",
    "Most of our tools for image processing in scipy are found in the `ndimage` subpackage:\n",
    "https://docs.scipy.org/doc/scipy-0.16.1/reference/ndimage.html\n",
    "\n",
    "-------------------------\n",
    "\n",
    "First, let's see if we can clean up the noisy green channel. A Gaussian filter will smooth the data in 2D (much like a running average), whereas a median filter will eliminate some salt-and-pepper noise while preserving the integrity of edges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Zoom in on a small region so we can clearly see the noise\n",
    "chunk = im_ch2[100:200, 100:200]\n",
    "\n",
    "# Use gaussian_filter to smooth out the image\n",
    "smooth = ndi.gaussian_filter(chunk, sigma=2)\n",
    "\n",
    "# Use median_filter to remove speckle but preserve sharp edges\n",
    "median = ndi.median_filter(chunk, size=3, mode='constant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "############### PLOTTING ################\n",
    "fig_image = plt.figure(figsize=(15, 10))\n",
    "ax1 = fig_image.add_subplot(131)\n",
    "ax2 = fig_image.add_subplot(132)\n",
    "ax3 = fig_image.add_subplot(133)\n",
    "ax1.imshow(chunk, cmap=plt.cm.gray)\n",
    "ax1.set_title('Original')\n",
    "ax2.imshow(smooth, cmap=plt.cm.gray)\n",
    "ax2.set_title('Gaussian Smoothing')\n",
    "ax3.imshow(median, cmap=plt.cm.gray)\n",
    "ax3.set_title('Median Filter');\n",
    "##########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "############### EXERCISE ###############\n",
    "# Selecting an appropriate filter size is important\n",
    "# for ensuring that you reject as much noise as possible without\n",
    "# losing too much detail.\n",
    "\n",
    "# Try changing the gaussian and median filter sizes to get an \n",
    "# intuitive sense of this tradeoff. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Baseline Removal**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cell_im = ndi.imread('BOb.png')[...,0] #load image\n",
    "\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.imshow(cell_im, cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that some regions of this image are brighter than others; how can we flatten the image? Similar to our approach with the 1D calcium signal, we can use a heavy low-pass filter to get a profile of the background, and then subtract or divide it from the image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "background = ndi.gaussian_filter(cell_im.astype(float), sigma=20)\n",
    "\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.imshow(background, cmap='gray');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Divide the background from the original image. \n",
    "# Using division effectively amplifies the signal in the dark regions of the\n",
    "# image, causing all cells to appear roughly equally bright\n",
    "bg_removed = cell_im / background\n",
    "\n",
    "################# PLOTTING ##################\n",
    "fig_image = plt.figure(figsize=(20, 15))\n",
    "ax1 = fig_image.add_subplot(131)\n",
    "ax2 = fig_image.add_subplot(132)\n",
    "ax1.imshow(cell_im, cmap=plt.cm.gray)\n",
    "ax2.imshow(bg_removed, cmap=plt.cm.gray);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "############### EXERCISE ###############\n",
    "# Try experimenting with different filter sizes in the background\n",
    "# removal process. What features of the image do you trade by\n",
    "# increasing or decreasing the size?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This image is pretty well cleaned up at this point. Let's say our goal now is to find regions of interest corresponding to individual cells. This can be a surprisingly difficult problem, and there are many ways to approach it. One simple way looks like:\n",
    "\n",
    "1. Manually select a threshold brightness that separates cells from background\n",
    "2. Find all contiguous regions \"blobs\" that are above threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "threshold = bg_removed.mean() + bg_removed.std()\n",
    "binary_img = bg_removed > threshold\n",
    "\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.imshow(binary_img);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# That's a start, but perhaps we can clean it up a bit?\n",
    "clean_bin_img = ndi.median_filter(binary_img, 10)\n",
    "\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.imshow(clean_bin_img);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Now we create a \"label\" array: the region over each \n",
    "# cell is given a unique integer value\n",
    "labels = scipy.ndimage.label(clean_bin_img)[0]\n",
    "\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.imshow(labels);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# From the label array, we can select individual cells from the original image:\n",
    "cells = [bg_removed[i, j] for i, j in ndi.find_objects(labels)]\n",
    "\n",
    "plt.imshow(cells[16], cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h2> Statistics </h2>\n",
    "\n",
    "The `scipy.stats` module contains two major parts:\n",
    "\n",
    "1. A set of classes representing [many continuous and discrete statistical distributions](https://docs.scipy.org/doc/scipy/reference/stats.html#continuous-distributions) (gaussian, binomial, poisson, etc.)\n",
    "2. A set of functions implementing [common statistical methods](https://docs.scipy.org/doc/scipy/reference/stats.html#statistical-functions) such as t-tests, ANOVA, correlation corfficients, etc.\n",
    "\n",
    "---------------------\n",
    "\n",
    "Let's begin by generating some random data. Scipy's distribution classes are designed such that each one implements the same basic interface (to the extent that this is possible). For example, we can generate random variables from any distribution by using its `rvs()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "\n",
    "# Pick 500 random values from a normal distribution centered at 5.0\n",
    "dist1 = stats.norm(loc=5.0, scale=1.2)\n",
    "data1 = dist1.rvs(size=500)\n",
    "\n",
    "# Pick 500 random values from a lognormal distribution\n",
    "dist2 = stats.lognorm(s=1, scale=3.2)\n",
    "data2 = dist2.rvs(size=500)\n",
    "\n",
    "# as a reminder, numpy already provides some very basic tools:\n",
    "print \"data1 mean: %f  stdev: %f\" % (data1.mean(), data1.std())\n",
    "print \"data2 mean: %f  stdev: %f\" % (data2.mean(), data2.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from matplotlib.pylab import axes, figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot histograms of the random values we generated\n",
    "ax = axes()\n",
    "bins = np.linspace(0, 15, 40)\n",
    "ax.hist(data1, bins=bins, color=(1.0, 0.6, 0.2, 0.3));\n",
    "ax.hist(data2, bins=bins, color=(0.2, 0.3, 1.0, 0.3));\n",
    "\n",
    "# Use the pdf() method to plot the probability density function of each distribution\n",
    "x = np.linspace(0, 15, 200)\n",
    "ax.plot(x, dist1.pdf(x) * 200, color=(1, 0.6, 0.2));\n",
    "ax.plot(x, dist2.pdf(x) * 200, color=(0.2, 0.3, 1.0));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just saw uses of the `rvs()` and `pdf()` methods of continuous distributions. The full list of methods available for each distribution can be found at the bottom of its documentation page. Take a moment to look at the documentation for [scipy.norm](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html#scipy.stats.norm) to get an idea of the methods available.\n",
    "\n",
    "-----------------\n",
    "\n",
    "Now let's try out a few of the functions available in scipy.stats. Start by having a peek at the [documentation](https://docs.scipy.org/doc/scipy/reference/stats.html#statistical-functions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Imagine data1 and data2 are repeated measurements taken under different conditions.\n",
    "# It sure looks like the results are different, but our reviewer is asking for a p-value.\n",
    "# How about a T-test?\n",
    "stats.ttest_ind(data1, data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's try a different approach..\n",
    "stats.ks_2samp(data1, data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "############### EXERCISE ###############\n",
    "#\n",
    "# Generate two random data sets and calculate the Pearson\n",
    "# correlation coefficient between them. How many comparisons do\n",
    "# you need to make before getting a false positive?\n",
    "#\n",
    "#  See: scipy.stats.norm and scipy.stats.pearsonr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Additional Material & Exercises </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using R statistical tools from Python\n",
    "\n",
    "Some of the highest quality, vetted statistical tools are written in R. The `rpy2` module makes it possible to call R functions from the comfort of your Python environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# to run this code, you need the rpy2 package \n",
    "# (this can be installed by running `conda install rpy2`)\n",
    "import rpy2.robjects.packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# install Jewell & Witten's LZeroSpikeInference package (if needed)\n",
    "import rpy2.robjects.packages as rpackages\n",
    "if not rpackages.isinstalled('LZeroSpikeInference'):\n",
    "    url = 'https://github.com/jewellsean/LZeroSpikeInference/archive/master.zip'\n",
    "    print(\"Downloading %s...\" % url)\n",
    "    import urllib2\n",
    "    response = urllib2.urlopen(url)\n",
    "    open('LZeroSpikeInference.zip', 'w').write(response.read())\n",
    "    \n",
    "    print(\"Extracting zip...\")\n",
    "    import zipfile\n",
    "    zip_ref = zipfile.ZipFile('LZeroSpikeInference.zip', 'r')\n",
    "    zip_ref.extractall('LZeroSpikeInference')\n",
    "    zip_ref.close()\n",
    "\n",
    "    print(\"Installing package to R...\")\n",
    "    \n",
    "    utils = rpackages.importr('utils')\n",
    "    utils.install_packages(\"./LZeroSpikeInference/LZeroSpikeInference-master\", repo=rpy2.robjects.NULL, type=\"source\")\n",
    "    \n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import rpy2.robjects.numpy2ri\n",
    "rpy2.robjects.numpy2ri.activate()\n",
    "lzsi = rpy2.robjects.packages.importr(\"LZeroSpikeInference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If you did not manage to fully filter the ca signal,\n",
    "# then you can load ours:\n",
    "with h5py.File('noisy_signal.h5', 'r') as f:\n",
    "    clean_ca_signal = np.array(f['clean_ca_signal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = clean_ca_signal[5000:6000] / clean_ca_signal[5000:6000].max() + 0.1\n",
    "fit = lzsi.estimateSpikes(data, **{'gam':0.97, 'lambda':1, 'type':\"ar1\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ax = axes((0, 0, 1.4, 1))\n",
    "ax.plot(data, label='data')\n",
    "ax.plot(fit[1], label='l0 estimate')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting events from electrophysiology trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Let's load up a slightly more complicated signal\n",
    "# This trace has both synaptic events and background depolarization due to an optogenetic stimulation\n",
    "\n",
    "with h5py.File('sweeps.h5', 'r') as f:\n",
    "    electrode_2 = np.array(f['acquisition/timeseries/data_00000/electrode_2'])\n",
    "    electrode_2_unit = f['acquisition/timeseries/data_00000/electrode_2'].attrs['unit']\n",
    "    time = np.array(f['acquisition/timeseries/data_00000/time'])\n",
    "    time_unit = f['acquisition/timeseries/data_00000/time'].attrs['unit']\n",
    "    \n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(time, electrode_2)\n",
    "ax.set_xlabel(time_unit), ax.set_ylabel(electrode_2_unit);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "################ EXERCISE ###############\n",
    "# Here are some tools\n",
    "\n",
    "# take median of first 500 elements and subtract from trace\n",
    "def zero_baseline(data, n=500):\n",
    "    med = np.median(data[0:n])\n",
    "    return data - med\n",
    "\n",
    "# filter: this function will function as a low, high, or band pass filter, but you will \n",
    "# most likely want to use it as a low pass filter for the signal above \n",
    "def filter_signal(data, sample_rate, cutoff_low = None, cutoff_high = None, order=5):\n",
    "    if cutoff_low is not None:\n",
    "        l = cutoff_low/(2*sample_rate)\n",
    "        if cutoff_high is not None:\n",
    "            filt_type = 'band'\n",
    "            h = cutoff_high/(2*sample_rate)\n",
    "            \n",
    "            filts = [l, h]\n",
    "        else:\n",
    "            filt_type = 'low'\n",
    "            filts = [l]\n",
    "    elif cutoff_high is not None:\n",
    "        filt_type = 'high'\n",
    "        h = cutoff_high/(2*sample_rate)\n",
    "        filts = [h]\n",
    "    else:\n",
    "        return 'Need some type of filter!'\n",
    "    \n",
    "    b, a = signal.butter(order, filts, btype = filt_type)\n",
    "    \n",
    "    filtered_data = signal.lfilter(b, a, data)\n",
    "    \n",
    "    return filtered_data\n",
    "    \n",
    "\n",
    "# sliding maximum filter to remove direct depolarization\n",
    "def sliding_max(data, win_len, smooth_win=500):\n",
    "    max_filt = ndi.maximum_filter(data, win_len)\n",
    "    smoothed = np.convolve(max_filt, np.ones(smooth_win)/smooth_win, mode='same')\n",
    "    return smoothed\n",
    "           \n",
    "\n",
    "# peak detection, takes in optional threshold arguments\n",
    "def find_peaks(data, pos_threshold = None, neg_threshold = None):\n",
    "    \n",
    "    if pos_threshold is None:\n",
    "        pos_threshold = np.mean(data) + 2.5*np.std(data)\n",
    "    if neg_threshold is None:\n",
    "        neg_threshold = np.mean(data) - 2.5*np.std(data)\n",
    "        \n",
    "    sig_deriv = np.diff(data)\n",
    "    \n",
    "    sig_deriv = (sig_deriv > 0).astype(np.byte) \n",
    "    \n",
    "    neg_peak_opts = np.argwhere(sig_deriv[1:] - sig_deriv[:-1] == 1) \n",
    "    pos_peak_opts = np.argwhere(sig_deriv[1:] - sig_deriv[:-1] == -1)\n",
    "    \n",
    "    pos = pos_peak_opts[data[pos_peak_opts] > pos_threshold]\n",
    "    neg = neg_peak_opts[data[neg_peak_opts] < neg_threshold]\n",
    "    \n",
    "    return pos, neg\n",
    "\n",
    "\n",
    "############# EXERCISE ###############\n",
    "## use the above functions and suggestions below to play with various filtering options\n",
    "## here is a start\n",
    "sample_rate = 1/(time[1] - time[0]) #get sampling rate from time vector\n",
    "\n",
    "zeroed = zero_baseline(electrode_2) #start by zeroing the baseline\n",
    "filtered = filter_signal(zeroed, sample_rate, cutoff_low = 1000) #add some filtering\n",
    "\n",
    "win = 3000 #adjust this to change smoothing\n",
    "max_filter = sliding_max(filtered, win)\n",
    "\n",
    "depolarization_subtracted = filtered[0:len(max_filter)] - max_filter\n",
    "\n",
    "pos, neg = find_peaks(depolarization_subtracted)\n",
    "####################################\n",
    "\n",
    "\n",
    "#### plotting\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(time[0:len(depolarization_subtracted)], depolarization_subtracted)\n",
    "ax.plot(time[0:len(max_filter)], max_filter)\n",
    "ax.legend(['filtered', 'maximum filter'])\n",
    "ax.scatter(time[neg], depolarization_subtracted[neg], color='r')\n",
    "ax.scatter(time[pos], depolarization_subtracted[pos], color='g')\n",
    "ax.set_xlabel('s')\n",
    "ax.set_ylabel('mV');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scikit-image packages contains several \"blob detection\" modules, and we will employ two of them here to attempt to classify cells in the above image:\n",
    "\n",
    "1. Difference of Gaussians: this method involves the subtraction of two gaussian filters (similar to the ones employed earlier). This is analagous to passband in signal processing, and in an image, will extract the edges of an object. \n",
    "\n",
    "2. Laplacian of Gaussians: A gaussian filter first acts to smooth the image, and a Laplacian (which takes the second derivative, $L_{xx} + L_{yy} $) performs edge detection, but with a positive response moving into the blob, and a negative response moving out of the blob. \n",
    "\n",
    "Read more here:\n",
    "\n",
    "http://scikit-image.org/docs/dev/api/skimage.feature.html#skimage.feature.blob_dog\n",
    "\n",
    "http://scikit-image.org/docs/dev/api/skimage.feature.html#skimage.feature.blob_log\n",
    "\n",
    "http://fourier.eng.hmc.edu/e161/lectures/gradient/node9.html\n",
    "\n",
    "http://fourier.eng.hmc.edu/e161/lectures/gradient/node8.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative approaches to detecting cell ROIs\n",
    "\n",
    "\n",
    "If our eventual goal is to find regions of interest corresponding to cells, one challenging factor may be the apparent difference in morphology between the cells in the images above. You will notice that some are filled (solid) while others appear to be rings. We can take advantage of a morphological operation, dilation, to essentially \"fill\" all rings. In dilation, each pixel value is replaced with the maximum pixel value of the region around it. This will act to both enlarge and fill the circle. The extent of this is determined by the size of the structuring element used for dilation. Although this operation will fill rings (good), it will artifically enlarge both cells and noise (bad). We can subsequently perform an erosion step with a structuring element of the same size. Erosion replaces each pixel with the minimum pixel value of its surroundings. Note that erosion is not quite the inverse of dilation: the rings that we filled will remain closed (they are surrounded by a neighborhood or bright pixels), but the size of the object will decrease. Both these operations can be done in ndimage with a single function: `grey_closing`\n",
    "\n",
    "http://utam.gg.utah.edu/tomo03/03_mid/HTML/node119.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Let's use our background-subtracted image fom above\n",
    "r = 5\n",
    "structure = np.fromfunction(lambda i,j: (i-r)**2+(j-r)**2 < r**2, (2*r+1, 2*r+1))\n",
    "closing_3 = ndi.morphology.grey_closing(bg_removed, structure=structure)\n",
    "closing_10 = ndi.morphology.grey_closing(bg_removed, size=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "################# PLOTTING ##################\n",
    "fig_image = plt.figure(figsize=(15, 10))\n",
    "ax1 = fig_image.add_subplot(131)\n",
    "ax2 = fig_image.add_subplot(132)\n",
    "ax3 = fig_image.add_subplot(133)\n",
    "\n",
    "ax1.imshow(bg_removed, cmap=plt.cm.gray)\n",
    "ax2.imshow(closing_3, cmap=plt.cm.gray)\n",
    "ax3.imshow(closing_10, cmap=plt.cm.gray)\n",
    "\n",
    "ax1.set_title('original')\n",
    "ax2.set_title('closing: size 3')\n",
    "ax3.set_title('closing: size 10');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "DoG = skimage.feature.blob_dog(closing_10, max_sigma=10, threshold=2) \n",
    "LoG = skimage.feature.blob_log(closing_10, max_sigma=10, threshold=0.5)\n",
    "# play around with optional parameters, max_sigma & threshold, to alter the size/brightness\n",
    "# of the blobs detected\n",
    "\n",
    "fig_image = plt.figure(figsize=(15, 10))\n",
    "ax1 = fig_image.add_subplot(121)\n",
    "ax2 = fig_image.add_subplot(122)\n",
    "\n",
    "\n",
    "ax1.imshow(cell_im) #back to our original images\n",
    "ax2.imshow(cell_im)\n",
    "\n",
    "\n",
    "for roi_DoG in DoG: \n",
    "    y_DoG = roi_DoG[0]\n",
    "    x_DoG = roi_DoG[1]\n",
    "    rad_DoG = roi_DoG[2] * 2**0.5\n",
    "    \n",
    "    ##### EXERCISE ########\n",
    "    \n",
    "    # filter by size\n",
    "    #if rad > threshold_pixels:\n",
    "        #color = 'r'\n",
    "        \n",
    "    #else:\n",
    "        #color = 'g'\n",
    "        \n",
    "    ax1.add_patch(plt.Circle((x_DoG,y_DoG), rad_DoG, color='r', fill=0))\n",
    "\n",
    "for roi_LoG in LoG:\n",
    "    y_LoG = roi_LoG[0]\n",
    "    x_LoG = roi_LoG[1]\n",
    "    rad_LoG = roi_LoG[2] * 2**0.5\n",
    "    \n",
    "    ##### EXERCISE ########\n",
    "    \n",
    "    # filter by size\n",
    "    # if rad > threshold_pixels:\n",
    "        # color = 'r'\n",
    "        \n",
    "    # else:\n",
    "        # continue\n",
    "    \n",
    "    \n",
    "    ax2.add_patch(plt.Circle((x_LoG,y_LoG), rad_LoG, color='r', fill=0))\n",
    "\n",
    "\n",
    "ax2.set_title('Laplacian of Gaussian')\n",
    "ax1.set_title('Difference of Gaussian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
